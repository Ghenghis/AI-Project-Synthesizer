"""
Test Generator for VIBE MCP Quality Pipeline

Automatically generates unit tests for generated code using:
- AST parsing to extract code structure
- LiteLLM router for intelligent test generation
- pytest for Python, jest for JavaScript/TypeScript
- Edge case detection and coverage analysis

This ensures all generated code has comprehensive test coverage.
"""

import ast
import asyncio
import json
import subprocess
import tempfile
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any

from src.core.config import get_settings
from src.llm.litellm_router import LiteLLMRouter


class TestType(Enum):
    """Types of tests to generate."""
    UNIT = "unit"
    INTEGRATION = "integration"
    E2E = "e2e"


@dataclass
class TestFunction:
    """Represents a generated test function."""
    name: str
    code: str
    description: str
    test_type: TestType
    target_function: str
    edge_cases: list[str]


@dataclass
class TestSuite:
    """Represents a complete test suite."""
    file_path: str
    test_file_path: str
    language: str
    imports: list[str]
    setup_code: str
    test_functions: list[TestFunction]
    coverage_estimate: float


class TestGenerator:
    """
    Intelligent test generation for VIBE MCP.

    Features:
    - AST-based code analysis
    - LLM-driven test logic generation
    - Edge case detection
    - Multiple framework support
    - Coverage estimation
    """

    def __init__(self):
        self.config = get_settings()
        self.llm_router = LiteLLMRouter()
        self.temp_dir = Path(tempfile.gettempdir()) / "vibe_mcp_tests"
        self.temp_dir.mkdir(exist_ok=True)

        # Test templates
        self.python_template = '''"""
Auto-generated tests for {module_name}
Generated by VIBE MCP Test Generator
"""

{imports}

{setup_code}

{test_functions}

if __name__ == "__main__":
    import pytest
    pytest.main([__file__])
'''

        self.javascript_template = '''/**
 * Auto-generated tests for {module_name}
 * Generated by VIBE MCP Test Generator
 */

{imports}

{setup_code}

{test_functions}
'''

    async def generate_tests(self, code: str, file_path: str, language: str = "python") -> TestSuite:
        """
        Generate comprehensive test suite for code.

        Args:
            code: The source code to test
            file_path: Original file path for context
            language: Programming language

        Returns:
            TestSuite with generated tests
        """
        # Parse code structure
        if language == "python":
            return await self._generate_python_tests(code, file_path)
        elif language in ["javascript", "typescript"]:
            return await self._generate_js_tests(code, file_path)
        else:
            raise ValueError(f"Unsupported language: {language}")

    async def _generate_python_tests(self, code: str, file_path: str) -> TestSuite:
        """Generate pytest tests for Python code."""
        try:
            # Parse AST
            tree = ast.parse(code)

            # Extract functions and classes
            functions = []
            classes = []

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append(self._extract_function_info(node))
                elif isinstance(node, ast.ClassDef):
                    classes.append(self._extract_class_info(node))

            # Generate tests for each function
            test_functions = []

            for func_info in functions:
                tests = await self._generate_function_tests(func_info, "python")
                test_functions.extend(tests)

            for class_info in classes:
                for method in class_info["methods"]:
                    tests = await self._generate_function_tests(method, "python", class_info["name"])
                    test_functions.extend(tests)

            # Determine imports needed
            imports = self._determine_python_imports(functions + classes)

            # Generate setup code
            setup_code = self._generate_python_setup(functions + classes)

            # Create test suite
            test_file_path = str(Path(file_path).with_name(f"test_{Path(file_path).stem}.py"))

            return TestSuite(
                file_path=file_path,
                test_file_path=test_file_path,
                language="python",
                imports=imports,
                setup_code=setup_code,
                test_functions=test_functions,
                coverage_estimate=self._estimate_coverage(functions, classes, test_functions)
            )

        except SyntaxError as e:
            raise TestGenerationError(f"Invalid Python syntax: {e}")

    async def _generate_js_tests(self, code: str, file_path: str) -> TestSuite:
        """Generate jest tests for JavaScript/TypeScript."""
        # For JS/TS, we'll use regex-based parsing (simpler than full AST)
        functions = self._extract_js_functions(code)
        classes = self._extract_js_classes(code)

        test_functions = []

        # Generate tests for functions
        for func_info in functions:
            tests = await self._generate_function_tests(func_info, "javascript")
            test_functions.extend(tests)

        # Generate tests for class methods
        for class_info in classes:
            for method in class_info["methods"]:
                tests = await self._generate_function_tests(method, "javascript", class_info["name"])
                test_functions.extend(tests)

        # Determine imports
        imports = self._determine_js_imports(functions + classes)

        # Generate setup code
        setup_code = self._generate_js_setup(functions + classes)

        # Create test suite
        test_file_path = str(Path(file_path).with_name(f"{Path(file_path).stem}.test.js"))

        return TestSuite(
            file_path=file_path,
            test_file_path=test_file_path,
            language="javascript",
            imports=imports,
            setup_code=setup_code,
            test_functions=test_functions,
            coverage_estimate=self._estimate_coverage(functions, classes, test_functions)
        )

    def _extract_function_info(self, node: ast.FunctionDef) -> dict[str, Any]:
        """Extract information from a Python function AST node."""
        info = {
            "name": node.name,
            "args": [arg.arg for arg in node.args.args],
            "returns": None,
            "docstring": ast.get_docstring(node) or "",
            "decorators": [d.id if isinstance(d, ast.Name) else str(d) for d in node.decorator_list],
            "is_async": isinstance(node, ast.AsyncFunctionDef)
        }

        # Try to infer return type from annotation
        if node.returns:
            info["returns"] = ast.unparse(node.returns)

        # Check for common patterns
        info["patterns"] = self._detect_function_patterns(node)

        return info

    def _extract_class_info(self, node: ast.ClassDef) -> dict[str, Any]:
        """Extract information from a Python class AST node."""
        methods = []

        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                methods.append(self._extract_function_info(item))

        return {
            "name": node.name,
            "methods": methods,
            "docstring": ast.get_docstring(node) or "",
            "bases": [base.id if isinstance(base, ast.Name) else str(base) for base in node.bases]
        }

    def _extract_js_functions(self, code: str) -> list[dict[str, Any]]:
        """Extract JavaScript functions using regex."""
        import re

        functions = []

        # Function declaration: function name() {}
        func_pattern = r'function\s+(\w+)\s*\([^)]*\)\s*\{'
        for match in re.finditer(func_pattern, code):
            functions.append({
                "name": match.group(1),
                "type": "function",
                "args": [],  # Simplified
                "docstring": "",
                "patterns": []
            })

        # Arrow function: const name = () => {}
        arrow_pattern = r'(?:const|let|var)\s+(\w+)\s*=\s*\([^)]*\)\s*=>'
        for match in re.finditer(arrow_pattern, code):
            functions.append({
                "name": match.group(1),
                "type": "arrow",
                "args": [],
                "docstring": "",
                "patterns": []
            })

        return functions

    def _extract_js_classes(self, code: str) -> list[dict[str, Any]]:
        """Extract JavaScript classes using regex."""
        import re

        classes = []

        # Class pattern
        class_pattern = r'class\s+(\w+)(?:\s+extends\s+(\w+))?\s*\{'

        for match in re.finditer(class_pattern, code):
            # Extract methods for this class
            class_start = match.start()
            class_code = self._extract_class_body(code, class_start)

            methods = self._extract_js_functions(class_code)

            classes.append({
                "name": match.group(1),
                "methods": methods,
                "docstring": "",
                "bases": [match.group(2)] if match.group(2) else []
            })

        return classes

    def _extract_class_body(self, code: str, start_pos: int) -> str:
        """Extract class body from position."""
        brace_count = 0
        in_class = False
        body_start = None

        for i, char in enumerate(code[start_pos:], start_pos):
            if char == '{':
                if not in_class:
                    in_class = True
                    body_start = i
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0 and in_class:
                    return code[body_start:i+1]

        return ""

    def _detect_function_patterns(self, node: ast.FunctionDef) -> list[str]:
        """Detect common patterns in function."""
        patterns = []

        # Check for database operations
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Attribute):
                    if child.func.attr in ["execute", "fetchall", "commit"]:
                        patterns.append("database")
                    elif child.func.attr in ["get", "post", "put", "delete"]:
                        patterns.append("http")
                    elif child.func.attr in ["open", "read", "write"]:
                        patterns.append("file_io")
                elif isinstance(child.func, ast.Name):
                    if child.func.id in ["print", "log"]:
                        patterns.append("logging")

        # Check for error handling
        if any(isinstance(n, ast.Raise) for n in node.body):
            patterns.append("error_handling")

        # Check for loops
        if any(isinstance(n, ast.For | ast.While) for n in node.body):
            patterns.append("iteration")

        return patterns

    async def _generate_function_tests(self, func_info: dict[str, Any], language: str, class_name: str | None = None) -> list[TestFunction]:
        """Generate tests for a specific function using LLM."""
        # Build prompt for LLM
        target = f"{class_name}.{func_info['name']}" if class_name else func_info['name']

        prompt = f"""Generate 3-5 comprehensive unit tests for the {language} function '{target}'.

Function signature:
- Name: {func_info['name']}
- Arguments: {', '.join(func_info['args'])}
- Return type: {func_info.get('returns', 'unknown')}
- Description: {func_info['docstring']}
- Patterns detected: {', '.join(func_info.get('patterns', []))}

Requirements:
1. Include happy path test with typical values
2. Test edge cases (null/None, empty, boundary values)
3. Test error conditions if applicable
4. Use descriptive test names
5. Include meaningful assertions that verify behavior
6. Use appropriate test framework ({'pytest' if language == 'python' else 'jest'})

Return JSON format:
{{
    "tests": [
        {{
            "name": "test_description",
            "description": "What this test verifies",
            "code": "full test code",
            "edge_cases": ["edge case 1", "edge case 2"]
        }}
    ]
}}"""

        try:
            # Get LLM response
            response = await self.llm_router.generate(
                prompt=prompt,
                model="claude-sonnet",  # Use Claude for code generation
                max_tokens=2000
            )

            # Parse JSON response
            try:
                data = json.loads(response)
                tests = []

                for test_data in data.get("tests", []):
                    test_func = TestFunction(
                        name=test_data["name"],
                        code=test_data["code"],
                        description=test_data["description"],
                        test_type=TestType.UNIT,
                        target_function=func_info["name"],
                        edge_cases=test_data.get("edge_cases", [])
                    )
                    tests.append(test_func)

                return tests

            except json.JSONDecodeError:
                # Fallback: generate basic tests
                return self._generate_fallback_tests(func_info, language, class_name)

        except Exception as e:
            print(f"LLM test generation failed: {e}")
            return self._generate_fallback_tests(func_info, language, class_name)

    def _generate_fallback_tests(self, func_info: dict[str, Any], language: str, class_name: str | None = None) -> list[TestFunction]:
        """Generate basic tests as fallback."""
        target = f"{class_name}.{func_info['name']}" if class_name else func_info['name']

        if language == "python":
            test_code = f"""def test_{func_info['name']}_basic():
    # Basic test for {target}
    result = {target}(...)
    assert result is not None"""
        else:
            test_code = f"""test('{func_info['name']}_basic', () => {{
    // Basic test for {target}
    const result = {target}(...);
    expect(result).toBeDefined();
}});"""

        return [TestFunction(
            name=f"test_{func_info['name']}_basic",
            code=test_code,
            description="Basic functionality test",
            test_type=TestType.UNIT,
            target_function=func_info['name'],
            edge_cases=[]
        )]

    def _determine_python_imports(self, functions_and_classes: list[dict[str, Any]]) -> list[str]:
        """Determine required imports for Python tests."""
        imports = ["import pytest"]

        # Check for async functions
        if any(f.get("is_async", False) for f in functions_and_classes if "is_async" in f):
            imports.append("import asyncio")

        # Check for specific patterns
        all_patterns = []
        for item in functions_and_classes:
            all_patterns.extend(item.get("patterns", []))

        if "database" in all_patterns:
            imports.append("from unittest.mock import Mock, patch")

        if "http" in all_patterns:
            imports.append("import requests_mock")

        if "file_io" in all_patterns:
            imports.append("import tempfile")
            imports.append("from pathlib import Path")

        return imports

    def _determine_js_imports(self, functions_and_classes: list[dict[str, Any]]) -> list[str]:
        """Determine required imports for JS tests."""
        imports = []

        # Check for patterns
        all_patterns = []
        for item in functions_and_classes:
            all_patterns.extend(item.get("patterns", []))

        if "http" in all_patterns:
            imports.append("import fetch from 'node-fetch';")

        return imports

    def _generate_python_setup(self, functions_and_classes: list[dict[str, Any]]) -> str:
        """Generate setup code for Python tests."""
        setup_parts = []

        # Add fixtures if needed
        all_patterns = []
        for item in functions_and_classes:
            all_patterns.extend(item.get("patterns", []))

        if "database" in all_patterns:
            setup_parts.append("""
@pytest.fixture
def mock_db():
    return Mock()
""")

        if "http" in all_patterns:
            setup_parts.append("""
@pytest.fixture
def mock_requests():
    with requests_mock.Mocker() as m:
        yield m
""")

        return "\n".join(setup_parts)

    def _generate_js_setup(self, functions_and_classes: list[dict[str, Any]]) -> str:
        """Generate setup code for JS tests."""
        # Jest doesn't need much setup
        return ""

    def _estimate_coverage(self, functions: list[dict[str, Any]], classes: list[dict[str, Any]], test_functions: list[TestFunction]) -> float:
        """Estimate test coverage based on generated tests."""
        total_items = len(functions) + sum(len(c["methods"]) for c in classes)
        tested_items = len({tf.target_function for tf in test_functions})

        if total_items == 0:
            return 0.0

        return min(100.0, (tested_items / total_items) * 100)

    def write_test_file(self, test_suite: TestSuite) -> str:
        """Write test suite to file."""
        # Format test functions
        test_code = "\n\n".join(tf.code for tf in test_suite.test_functions)

        if test_suite.language == "python":
            content = self.python_template.format(
                module_name=Path(test_suite.file_path).stem,
                imports="\n".join(test_suite.imports),
                setup_code=test_suite.setup_code,
                test_functions=test_code
            )
        else:
            content = self.javascript_template.format(
                module_name=Path(test_suite.file_path).stem,
                imports="\n".join(test_suite.imports),
                setup_code=test_suite.setup_code,
                test_functions=test_code
            )

        # Write to temp file
        test_file = self.temp_dir / Path(test_suite.test_file_path).name
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(content)

        return str(test_file)

    async def run_tests(self, test_file_path: str) -> dict[str, Any]:
        """Run generated tests and return results."""
        test_file = Path(test_file_path)

        if test_file.suffix == ".py":
            # Run pytest
            cmd = ["pytest", str(test_file), "--json-report", "--json-report-file=-"]
        else:
            # Run jest
            cmd = ["npx", "jest", str(test_file), "--json", "--outputFile=-"]

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=60,
                cwd=test_file.parent
            )

            if result.stdout:
                try:
                    return json.loads(result.stdout)
                except json.JSONDecodeError:
                    return {"status": "completed", "output": result.stdout}

            return {"status": "failed", "error": result.stderr}

        except subprocess.TimeoutExpired:
            return {"status": "timeout"}
        except FileNotFoundError:
            return {"status": "tool_not_found"}

    def generate_report(self, test_suite: TestSuite) -> str:
        """Generate a human-readable test report."""
        report = []
        report.append("=" * 60)
        report.append("TEST GENERATION REPORT")
        report.append("=" * 60)
        report.append(f"Source file: {test_suite.file_path}")
        report.append(f"Test file: {test_suite.test_file_path}")
        report.append(f"Language: {test_suite.language}")
        report.append(f"Estimated coverage: {test_suite.coverage_estimate:.1f}%")
        report.append(f"Tests generated: {len(test_suite.test_functions)}")
        report.append("")

        # Test summary
        report.append("GENERATED TESTS:")
        for test in test_suite.test_functions:
            report.append(f"  â€¢ {test.name}")
            report.append(f"    {test.description}")
            if test.edge_cases:
                report.append(f"    Edge cases: {', '.join(test.edge_cases)}")
        report.append("")

        # Instructions
        report.append("NEXT STEPS:")
        report.append(f"1. Review generated tests in: {test_suite.test_file_path}")
        report.append("2. Run tests to verify they pass")
        report.append("3. Add additional edge cases as needed")
        report.append("4. Update test assertions for specific requirements")

        report.append("\n" + "=" * 60)

        return "\n".join(report)


class TestGenerationError(Exception):
    """Raised when test generation fails."""
    pass


# CLI interface for testing
if __name__ == "__main__":
    import sys

    async def main():
        generator = TestGenerator()

        if len(sys.argv) > 1:
            # Generate tests for file
            file_path = Path(sys.argv[1])
            if file_path.exists():
                with open(file_path) as f:
                    code = f.read()

                language = "python" if file_path.suffix == ".py" else "javascript"
                test_suite = await generator.generate_tests(code, str(file_path), language)

                # Write test file
                test_file = generator.write_test_file(test_suite)
                print(f"Generated test file: {test_file}")
                print("\n" + generator.generate_report(test_suite))

                # Run tests
                print("\nRunning tests...")
                results = await generator.run_tests(test_file)
                print(f"Test results: {results}")
            else:
                print(f"File not found: {file_path}")
        else:
            # Demo generation
            demo_code = '''
def calculate_total(items, tax_rate=0.1):
    """Calculate total cost including tax."""
    if not items:
        return 0

    subtotal = sum(items)
    total = subtotal * (1 + tax_rate)
    return round(total, 2)

class Calculator:
    def __init__(self):
        self.history = []

    def add(self, a, b):
        result = a + b
        self.history.append(f"{a} + {b} = {result}")
        return result

    def divide(self, a, b):
        if b == 0:
            raise ValueError("Cannot divide by zero")
        return a / b
'''
            test_suite = await generator.generate_tests(demo_code, "demo.py", "python")
            test_file = generator.write_test_file(test_suite)
            print(f"Generated test file: {test_file}")
            print("\n" + generator.generate_report(test_suite))

    asyncio.run(main())
