# ============================================
# AI Project Synthesizer - Environment Configuration
# ============================================
# Copy this file to .env and fill in your values
# NEVER commit .env to version control!
# ============================================

# ============================================
# APPLICATION SETTINGS
# ============================================
APP_NAME=AI Project Synthesizer
APP_ENV=development
DEBUG=true
LOG_LEVEL=INFO

# ============================================
# SERVER SETTINGS
# ============================================
SERVER_HOST=localhost
SERVER_PORT=8000

# ============================================
# PLATFORM API KEYS (Required: at least GitHub)
# ============================================

# GitHub - Required for repository discovery
# Get token: https://github.com/settings/tokens
# Required scopes: repo (read), read:packages
GITHUB_TOKEN=ghp_your_token_here

# GitLab - Optional
# Get token: https://gitlab.com/-/user_settings/personal_access_tokens
GITLAB_TOKEN=
GITLAB_URL=https://gitlab.com

# HuggingFace - Optional but recommended
# Get token: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=hf_your_token_here

# Kaggle - Optional
# Get credentials: https://www.kaggle.com/account → API → Create New Token
KAGGLE_USERNAME=
KAGGLE_KEY=

# Semantic Scholar - Optional (improves paper search)
# Get key: https://www.semanticscholar.org/product/api
SEMANTIC_SCHOLAR_API_KEY=

# ============================================
# LLM CONFIGURATION - Multi-Provider Support
# ============================================

# Preferred LLM Provider
# LOCAL: ollama, lmstudio, localai, vllm, textgen_webui, koboldai, llamacpp
# CLOUD: openai, anthropic, groq, together, openrouter, mistral, deepseek, cohere, fireworks
# Default: ollama (local, privacy-focused)
LLM_PREFERRED_PROVIDER=ollama

# Model Size Preference - Controls which models to use
# Options: tiny (< 2B), small (2-4B), medium (4-7B), large (8-14B)
# Default: medium (7B models - best balance of performance and resources)
LLM_MODEL_SIZE_PREFERENCE=medium

# Enable fallback between providers
LLM_FALLBACK_ENABLED=true

# ============================================
# LOCAL PROVIDERS
# ============================================

# --- Ollama (Recommended) ---
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL_TINY=qwen2.5-coder:1.5b-instruct
OLLAMA_MODEL_SMALL=qwen2.5-coder:3b-instruct
OLLAMA_MODEL_MEDIUM=qwen2.5-coder:7b-instruct-q8_0
OLLAMA_MODEL_LARGE=qwen2.5-coder:14b-instruct-q4_K_M

# --- LM Studio ---
LMSTUDIO_HOST=http://localhost:1234
LMSTUDIO_API_KEY=lm-studio
LMSTUDIO_ENABLED=true
LMSTUDIO_MODEL_TINY=your-tiny-model
LMSTUDIO_MODEL_SMALL=your-small-model
LMSTUDIO_MODEL_MEDIUM=your-medium-model
LMSTUDIO_MODEL_LARGE=your-large-model

# --- LocalAI ---
LOCALAI_HOST=http://localhost:8080
LOCALAI_ENABLED=false
LOCALAI_MODEL=gpt-3.5-turbo

# --- vLLM ---
VLLM_HOST=http://localhost:8000
VLLM_ENABLED=false
VLLM_MODEL=default

# --- Text Generation WebUI ---
TEXTGEN_HOST=http://localhost:5000
TEXTGEN_ENABLED=false
TEXTGEN_MODEL=default

# --- KoboldAI ---
KOBOLDAI_HOST=http://localhost:5001
KOBOLDAI_ENABLED=false

# --- llama.cpp Server ---
LLAMACPP_HOST=http://localhost:8080
LLAMACPP_ENABLED=false

# ============================================
# CLOUD PROVIDERS (Optional)
# ============================================

# --- OpenAI ---
# Get key: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your_key_here
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_ENABLED=false

# --- Anthropic ---
# Get key: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your_key_here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_ENABLED=false

# --- Groq (Fast Inference) ---
# Get key: https://console.groq.com/
GROQ_API_KEY=gsk_your_key_here
GROQ_MODEL=llama-3.1-70b-versatile
GROQ_ENABLED=false

# --- Together AI ---
# Get key: https://api.together.xyz/
TOGETHER_API_KEY=your_key_here
TOGETHER_MODEL=meta-llama/Llama-3-70b-chat-hf
TOGETHER_ENABLED=false

# --- OpenRouter (Multi-Model Gateway) ---
# Get key: https://openrouter.ai/
OPENROUTER_API_KEY=sk-or-your_key_here
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
OPENROUTER_ENABLED=false

# --- Mistral AI ---
# Get key: https://console.mistral.ai/
MISTRAL_API_KEY=your_key_here
MISTRAL_MODEL=mistral-large-latest
MISTRAL_ENABLED=false

# --- DeepSeek ---
# Get key: https://platform.deepseek.com/
DEEPSEEK_API_KEY=your_key_here
DEEPSEEK_MODEL=deepseek-coder
DEEPSEEK_ENABLED=false

# --- Cohere ---
# Get key: https://dashboard.cohere.com/
COHERE_API_KEY=your_key_here
COHERE_MODEL=command-r-plus
COHERE_ENABLED=false

# --- Fireworks AI ---
# Get key: https://fireworks.ai/
FIREWORKS_API_KEY=your_key_here
FIREWORKS_MODEL=accounts/fireworks/models/llama-v3p1-70b-instruct
FIREWORKS_ENABLED=false

# ============================================
# ROUTING CONFIGURATION
# ============================================
# Threshold for routing to cloud (0.0-1.0)
# Higher = more local, lower = more cloud
CLOUD_ROUTING_THRESHOLD=0.7

# ============================================
# CACHE CONFIGURATION
# ============================================
CACHE_ENABLED=true
CACHE_TTL_SECONDS=3600
CACHE_DIR=.cache

# Redis (optional, for distributed caching)
REDIS_URL=redis://localhost:6379/0

# ============================================
# OUTPUT CONFIGURATION
# ============================================
DEFAULT_OUTPUT_DIR=./output
TEMP_DIR=./temp

# ============================================
# SYNTHESIS SETTINGS
# ============================================
MAX_REPOS_PER_SYNTHESIS=10
MAX_CONCURRENT_CLONES=3
CLONE_DEPTH=1

# ============================================
# RATE LIMITING
# ============================================
GITHUB_RATE_LIMIT_BUFFER=100
REQUEST_TIMEOUT_SECONDS=30
MAX_RETRIES=3

# ============================================
# LOGGING
# ============================================
LOG_FORMAT=json
LOG_FILE=logs/synthesizer.log
LOG_ROTATION=10MB
LOG_RETENTION=7
